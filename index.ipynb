{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fabea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tensorflow import ImageDataGenerator\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# ===============================================\n",
    "#   1) Data Cleaning & Exploratory Analysis\n",
    "# ===============================================\n",
    "\n",
    "SOURCE_DIR = \"/content/Data\"\n",
    "\n",
    "CLASSES = [\"Fatigue\", \"NonFatigue\"]\n",
    "\n",
    "BASE_OUTPUT = \"/content/splitted\"\n",
    "TRAIN_DIR = os.path.join(BASE_OUTPUT, \"train\")\n",
    "TEST_DIR = os.path.join(BASE_OUTPUT, \"test\")\n",
    "PREDICT_DIR = os.path.join(BASE_OUTPUT, \"predict\")\n",
    "\n",
    "for path in [TRAIN_DIR, TEST_DIR, PREDICT_DIR]:\n",
    "    for cls in CLASSES:\n",
    "        os.makedirs(os.path.join(path, cls), exist_ok=True)\n",
    "\n",
    "for cls in CLASSES:\n",
    "    class_dir = os.path.join(SOURCE_DIR, cls)\n",
    "    images = [img for img in os.listdir(class_dir)\n",
    "    if img.lower().endswith(('jpg', 'jpeg', 'png'))]\n",
    "\n",
    "    train_imgs, test_imgs = train_test_split(images, test_size=0.2, random_state=42)\n",
    "\n",
    "    for img in train_imgs:\n",
    "        shutil.copy(os.path.join(class_dir, img),\n",
    "                    os.path.join(TRAIN_DIR, cls, img))\n",
    "\n",
    "    for img in test_imgs:\n",
    "        shutil.copy(os.path.join(class_dir, img),\n",
    "                    os.path.join(TEST_DIR, cls, img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32671607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "#   2) Preprocessing & Data Augmentation\n",
    "# ============================================\n",
    "\n",
    "\n",
    "#   1) Transformations\n",
    "\n",
    "IMG_SIZE = 224\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "\n",
    "#   2) Custom Dataset\n",
    "\n",
    "class FatigueDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        classes = [\"Fatigue\", \"NonFatigue\"]   # 0 - 1\n",
    "\n",
    "        for label, folder in enumerate(classes):\n",
    "            folder_path = os.path.join(root_dir, folder)\n",
    "\n",
    "            for img_name in os.listdir(folder_path):\n",
    "                img_path = os.path.join(folder_path, img_name)\n",
    "\n",
    "                if img_name.lower().endswith((\"jpg\", \"jpeg\", \"png\")):\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "#   3) Loaders\n",
    "\n",
    "train_dataset = FatigueDataset(root_dir=\"/content/splitted/train\", transform=train_transforms)\n",
    "test_dataset  = FatigueDataset(root_dir=\"/content/splitted/test\",  transform=test_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"Train samples:\", len(train_dataset))\n",
    "print(\"Test samples:\", len(test_dataset))\n",
    "\n",
    "\n",
    "#   4) Test Loader\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    print(\"Images batch shape:\", images.shape)     # [32, 3, 224, 224]\n",
    "    print(\"Labels batch shape:\", labels.shape)     # [32]\n",
    "    break\n",
    "\n",
    "\"\"\"### **TensorFlow + ImageDataGenerator** (Option 2)\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Make a folder safely. Won't throw error if it already exists\n",
    "def safe_makedirs(path):\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Check if a file is an image based on its extension\n",
    "def is_image_file(p: Path):\n",
    "    return p.suffix.lower() in {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}\n",
    "\n",
    "\n",
    "# Load an image with OpenCV and resize it\n",
    "# Returns None if the image can't be read\n",
    "def read_image_cv2(path, target_size=(224, 224)):\n",
    "    img = cv2.imdecode(np.fromfile(str(path), dtype=np.uint8), cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        return None\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, target_size)\n",
    "    return img\n",
    "\n",
    "\n",
    "# Look at the dataset folder and figure out classes\n",
    "def analyze_dataset_structure(dataset_path):\n",
    "    p = Path(dataset_path)\n",
    "\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Dataset path not found: {dataset_path}\")\n",
    "\n",
    "    # Get class folder names\n",
    "    classes = [d.name for d in p.iterdir() if d.is_dir()]\n",
    "\n",
    "    # Case: no folders, all images in one folder\n",
    "    if not classes:\n",
    "        classes = ['images']\n",
    "        all_images = [f for f in p.iterdir() if f.is_file() and is_image_file(f)]\n",
    "        return classes, {'images': all_images}\n",
    "\n",
    "    # Case: folders exist, each folder is a class\n",
    "    class_files = {}\n",
    "    for cls in classes:\n",
    "        cls_path = p / cls\n",
    "        files = [f for f in cls_path.rglob('*') if f.is_file() and is_image_file(f)]\n",
    "        class_files[cls] = sorted(files)\n",
    "\n",
    "    return sorted(classes), class_files\n",
    "\n",
    "\n",
    "# Remove images that are corrupted or too small\n",
    "# Moves them to a separate folder\n",
    "def remove_corrupted_images(class_files, progress_callback=None):\n",
    "    corrupted_dir = Path(\"dataset_corrupted\")\n",
    "    safe_makedirs(corrupted_dir)\n",
    "\n",
    "    report = {\"total_checked\": 0, \"corrupted_count\": 0, \"corrupted_files\": []}\n",
    "    cleaned = {}\n",
    "\n",
    "    for cls, files in class_files.items():\n",
    "        cleaned[cls] = []\n",
    "\n",
    "        for f in files:\n",
    "            report[\"total_checked\"] += 1\n",
    "\n",
    "            try:\n",
    "                img = read_image_cv2(f)\n",
    "                # Skip images that are too small or unreadable\n",
    "                if img is None or img.shape[0] < 10 or img.shape[1] < 10:\n",
    "                    raise ValueError(\"Invalid image\")\n",
    "                cleaned[cls].append(f)\n",
    "\n",
    "            except:\n",
    "                report[\"corrupted_count\"] += 1\n",
    "                report[\"corrupted_files\"].append(str(f))\n",
    "                try:\n",
    "                    shutil.move(str(f), corrupted_dir / f.name)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            # Optional callback to show progress in a GUI\n",
    "            if progress_callback:\n",
    "                progress_callback(report[\"total_checked\"])\n",
    "\n",
    "    return cleaned, report\n",
    "\n",
    "\n",
    "# Make a grid of sample images from each class\n",
    "def generate_sample_grid(class_files, samples_per_class=4, out_path=\"samples_grid.png\", img_size=(224, 224)):\n",
    "    classes = list(class_files.keys())\n",
    "    rows = len(classes)\n",
    "    cols = samples_per_class\n",
    "    cell_w, cell_h = img_size\n",
    "\n",
    "    grid_w = cols * cell_w\n",
    "    grid_h = rows * cell_h\n",
    "\n",
    "    canvas = Image.new('RGB', (grid_w, grid_h), (30, 30, 30))\n",
    "\n",
    "    for i, cls in enumerate(classes):\n",
    "        files = class_files[cls][:samples_per_class]\n",
    "\n",
    "        for j in range(cols):\n",
    "            if j < len(files):\n",
    "                try:\n",
    "                    img = Image.open(files[j]).convert('RGB').resize((cell_w, cell_h))\n",
    "                except:\n",
    "                    img = Image.new('RGB', (cell_w, cell_h), (50, 50, 50))\n",
    "            else:\n",
    "                img = Image.new('RGB', (cell_w, cell_h), (80, 80, 80))\n",
    "\n",
    "            canvas.paste(img, (j * cell_w, i * cell_h))\n",
    "\n",
    "    canvas.save(out_path)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# Build data generators for training and validation\n",
    "def build_generators(data_dir, classes, img_size=(224, 224), batch_size=32, val_split=0.2, seed=42):\n",
    "    # Generator for training images with augmentation\n",
    "    train_aug = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=15,\n",
    "        width_shift_range=0.05,\n",
    "        height_shift_range=0.05,\n",
    "        zoom_range=0.08,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=val_split\n",
    "    )\n",
    "\n",
    "    # Generator for validation images (no augmentation)\n",
    "    val_aug = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=val_split\n",
    "    )\n",
    "\n",
    "    # Training set generator\n",
    "    train_gen = train_aug.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        classes=classes,\n",
    "        class_mode='binary' if len(classes) == 2 else 'categorical',\n",
    "        subset='training',\n",
    "        shuffle=True,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    # Validation set generator\n",
    "    val_gen = val_aug.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        classes=classes,\n",
    "        class_mode='binary' if len(classes) == 2 else 'categorical',\n",
    "        subset='validation',\n",
    "        shuffle=False,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    return train_gen, val_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b064dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "#   3) Model Design\n",
    "# ============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846860ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "#   4) Model Training, Saving & Evaluation\n",
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee32772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "#   5) Testing, Inference\n",
    "# ============================"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
